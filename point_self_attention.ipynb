{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers \n",
    "import time\n",
    "import provider\n",
    "# import dataset\n",
    "import glob\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices=tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES=glob.glob(\"F:\\点云深度学习代码\\Dataset\\modelnet40_ply_hdf5_2048\\*train*.h5\")\n",
    "TEST_FILES=glob.glob(\"F:\\点云深度学习代码\\Dataset\\modelnet40_ply_hdf5_2048\\*test*.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(keras.Model):\n",
    "    def __init__(self,out_dim,k_head):\n",
    "       super(self_attention,self).__init__()\n",
    "       self.out_dim=out_dim\n",
    "       self.k=k_head\n",
    "    def build(self,input_shape):\n",
    "        self.convq=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.convk=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.convv=[layers.Conv2D(self.out_dim,1,1) for i in range(self.k)]\n",
    "        self.Dense_out=layers.Dense(self.out_dim,activation=\"relu\")\n",
    "        self.norm=keras.layers.BatchNormalization()\n",
    "        self.soft=keras.layers.Softmax()\n",
    "    def call(self,input_feature,training=True):\n",
    "        out=[]\n",
    "        for i in range(self.k):\n",
    "            q=self.convq[i](input_feature)\n",
    "            v=self.convv[i](input_feature)\n",
    "            k=self.convk[i](input_feature)\n",
    "            q=tf.reduce_mean(q,axis=-2,keepdims=True)\n",
    "            # q=tf.tile(q,[1,1,tf.shape(v)[2],1])\n",
    "            k=tf.transpose(k,[0,1,3,2])\n",
    "            qk=tf.matmul(q,k)/tf.sqrt(tf.cast(self.out_dim,tf.float32))\n",
    "            out.append(tf.matmul(self.soft(qk),v))\n",
    "        out=tf.concat(out,axis=-1)\n",
    "        out=self.norm(self.Dense_out(tf.squeeze(out,axis=-2)),training=training)\n",
    "        # out=keras.layers.Dropout(0.1)(out,training=training)\n",
    "        return  out \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(xyz,feature):\n",
    "    num=tf.shape(xyz)[1]\n",
    "    return xyz[:,0:tf.cast(num/2,tf.int32),:],feature[:,0:tf.cast(num/2,tf.int32),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pointcloud_class(keras.Model):\n",
    "    def __init__(self,class_num=10):\n",
    "        super(pointcloud_class,self).__init__()\n",
    "        self.class_num=class_num\n",
    "    def build(self,inputshape):\n",
    "        self.n=7\n",
    "        self.Dense1=keras.layers.Dense(64,activation=\"relu\") \n",
    "        self.Dense2=keras.layers.Dense(128,activation=\"relu\") \n",
    "        self.norm1=keras.layers.BatchNormalization()\n",
    "        self.norm2=keras.layers.BatchNormalization()\n",
    "        self.norm3=keras.layers.BatchNormalization()\n",
    "        self.norm4=keras.layers.BatchNormalization()\n",
    "        self.self_attention=[self_attention(128,1) for i in range(self.n+1)]\n",
    "        # self.Dense3=layers.Dense(256,activation=\"relu\")\n",
    "        # self.Dense4=layers.Dense(128,activation=\"relu\")\n",
    "        self.Dense5=layers.Dense(self.class_num)\n",
    "        self.soft=keras.layers.Softmax()\n",
    "    def call(self , points_xyz,training=True):\n",
    "        Density=utils.kernel_density_estimation(points_xyz,1.0)\n",
    "        featrue=tf.concat([points_xyz,Density],axis=-1)\n",
    "        featrue=self.Dense1(points_xyz)\n",
    "        featrue=self.norm1(featrue,training=training)\n",
    "        featrue=self.Dense2(featrue)\n",
    "        featrue=self.norm2(featrue,training=training)\n",
    "        newpoints=points_xyz\n",
    "        for i in range(self.n):\n",
    "            new_points1,new_feature = random_sample(newpoints,featrue)\n",
    "            gouped_xyz,new_points,idx=utils.grouping(featrue,16,newpoints,new_points1)\n",
    "            featrue=self.self_attention[i](new_points,training=training)\n",
    "            newpoints=new_points1\n",
    "        grouped_xyz, new_points, idx=utils.grouping_all(featrue,newpoints)\n",
    "        out=self.self_attention[self.n](new_points,training=training)\n",
    "        out=tf.squeeze(out,axis=1)\n",
    "        # out=self.Dense3(out)\n",
    "        # out=self.norm3(out,training=training)\n",
    "        # out=layers.Dropout(0.2)(out,training=training)\n",
    "        # out=self.Dense4(out)\n",
    "        # out=self.norm4(out,training=training)\n",
    "        # out=layers.Dropout(0.2)(out,training=training)\n",
    "        out=self.Dense5(out)\n",
    "        out=self.soft(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule=keras.optimizers.schedules.ExponentialDecay(0.001,100000,0.7,staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer=keras.optimizers.Adam(0.001)\n",
    "loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model=pointcloud_class(40)\n",
    "mtric1=keras.metrics.SparseCategoricalAccuracy()\n",
    "mtric2=keras.metrics.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FOUT = open('log_train2.txt', 'a+')\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[];label=[]\n",
    "\n",
    "for i in range(len(TRAIN_FILES)):\n",
    "    data1,label1=provider.loadDataFile(TRAIN_FILES[i])\n",
    "    data.append(data1[:,0:1024,:])\n",
    "    label.append(label1)\n",
    "data=np.concatenate(data,axis=0)\n",
    "label=np.concatenate(label,axis=0)\n",
    "datasets=tf.data.Dataset.from_tensor_slices((data,label))\n",
    "datasets=datasets.shuffle(buffer_size=1024)\n",
    "func=lambda x,y:(tf.random.shuffle(x) ,y)\n",
    "datasets=datasets.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "datasets=datasets.batch(64)\n",
    "# datasets.cache()\n",
    "\n",
    "datasets=datasets.prefetch(1)\n",
    "\n",
    "data=[];label=[]\n",
    "\n",
    "for i in range(len(TEST_FILES)):\n",
    "    data1,label1=provider.loadDataFile(TEST_FILES[i])\n",
    "    data.append(data1[:,0:1024,:])\n",
    "    label.append(label1)\n",
    "data=np.concatenate(data,axis=0)\n",
    "label=np.concatenate(label,axis=0)\n",
    "test_datasets=tf.data.Dataset.from_tensor_slices((data,label))\n",
    "test_datasets=test_datasets.shuffle(buffer_size=1024)\n",
    "func=lambda x,y:(tf.random.shuffle(x) ,y)\n",
    "test_datasets=test_datasets.map(func,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_datasets=test_datasets.batch(64)\n",
    "# datasets.cache()\n",
    "test_datasets=test_datasets.prefetch(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shuffle(data):\n",
    "#     shuffed_data= np.zeros(data.shape, dtype=np.float32)\n",
    "#     for k in range(data.shape[0]):\n",
    "#         shuffed_data[k,...]=np.random.shuffle(data[k,...])\n",
    "#     return shuffed_data\n",
    "@tf.function\n",
    "def callmodel(input,current_label):\n",
    "    with tf.GradientTape() as tape:\n",
    "            logits=model(input,training=True)\n",
    "            if model.losses :\n",
    "                regularization_loss=tf.math.add_n(model.losses)\n",
    "            else:\n",
    "                regularization_loss=0\n",
    "            loss_value = loss(current_label, logits)+regularization_loss\n",
    "            # print(loss_value)\n",
    "    grads=tape.gradient(loss_value,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "    mtric1.update_state(current_label, logits)\n",
    "    mtric2.update_state(current_label, logits)\n",
    "@tf.function\n",
    "def evaluate_model(input,current_label):\n",
    "    logits=model(input,training=False)\n",
    "    mtric1.update_state(current_label, logits)\n",
    "    mtric2.update_state(current_label, logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffledata(data):\n",
    "    idx=np.arange(data.shape[1])\n",
    "    for k in range(data.shape[0]):\n",
    "        np.random.shuffle(idx)\n",
    "        data[k,...]=data[k,idx,:]\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(epoch):\n",
    "    # train_file_idxs = np.arange(0, len(TRAIN_FILES))\n",
    "    # np.random.shuffle(train_file_idxs)\n",
    "    NUM_POINT=1024\n",
    "    BATCH_SIZE=64\n",
    "    global BATCH\n",
    "    for step,(current_data,current_label) in enumerate(datasets):\n",
    "        # print(current_data.shape[0])\n",
    "        BATCH=BATCH+1\n",
    "        ckpt.batch.assign_add(1)\n",
    "        # Augment batched point clouds by rotation and jittering\n",
    "        rotated_data = provider.rotate_point_cloud(current_data.numpy())\n",
    "        jittered_data = provider.jitter_point_cloud(rotated_data)\n",
    "        jittered_data= jittered_data.astype(np.float32)\n",
    "        jittered_data=shuffledata(jittered_data)\n",
    "        # jittered_data=shuffle(jittered_data)\n",
    "        # print(jittered_data.dtype)\n",
    "        # print(jittered_data.shape)\n",
    "        tf.keras.backend.set_value(optimizer.lr, lr_schedule(BATCH*BATCH_SIZE))\n",
    "        callmodel(jittered_data,current_label)\n",
    "        # pred_val = np.argmax(logits, 1)\n",
    "        # correct = np.sum(pred_val == current_label[start_idx:end_idx])\n",
    "        # total_correct += correct\n",
    "        # total_seen += BATCH_SIZE\n",
    "        # # print(loss_value)\n",
    "        # loss_sum += float(loss_value)\n",
    "def test_one_epoch(epoch):\n",
    "    for setp ,(data,label) in enumerate(test_datasets):\n",
    "        evaluate_model(data,label)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt=tf.train.Checkpoint(model=model,opti=optimizer,batch=tf.Variable(1))\n",
    "ckpt_mana=tf.train.CheckpointManager(ckpt,\"pointcloud_class\",max_to_keep=3)\n",
    "epochs=200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "-----0------\n",
      "train_mean loss: 0.528037\n",
      "train_accuracy: 0.830940\n",
      "test_mean loss: 0.677870\n",
      "test_accuracy: 0.801053\n",
      "\n",
      "Start of epoch 1\n",
      "-----1------\n",
      "train_mean loss: 0.494372\n",
      "train_accuracy: 0.836585\n",
      "test_mean loss: 0.664150\n",
      "test_accuracy: 0.805916\n",
      "\n",
      "Start of epoch 2\n",
      "-----2------\n",
      "train_mean loss: 0.485697\n",
      "train_accuracy: 0.838211\n",
      "test_mean loss: 0.665630\n",
      "test_accuracy: 0.797002\n",
      "\n",
      "Start of epoch 3\n",
      "-----3------\n",
      "train_mean loss: 0.488941\n",
      "train_accuracy: 0.834756\n",
      "test_mean loss: 0.693196\n",
      "test_accuracy: 0.790924\n",
      "\n",
      "Start of epoch 4\n",
      "-----4------\n",
      "train_mean loss: 0.490254\n",
      "train_accuracy: 0.837906\n",
      "test_mean loss: 0.672518\n",
      "test_accuracy: 0.796191\n",
      "\n",
      "Start of epoch 5\n",
      "-----5------\n",
      "train_mean loss: 0.487702\n",
      "train_accuracy: 0.837602\n",
      "test_mean loss: 0.662927\n",
      "test_accuracy: 0.805511\n",
      "\n",
      "Start of epoch 6\n",
      "-----6------\n",
      "train_mean loss: 0.480528\n",
      "train_accuracy: 0.839634\n",
      "test_mean loss: 0.654708\n",
      "test_accuracy: 0.807942\n",
      "\n",
      "Start of epoch 7\n",
      "-----7------\n",
      "train_mean loss: 0.463320\n",
      "train_accuracy: 0.839736\n",
      "test_mean loss: 0.639140\n",
      "test_accuracy: 0.805105\n",
      "\n",
      "Start of epoch 8\n",
      "-----8------\n",
      "train_mean loss: 0.467844\n",
      "train_accuracy: 0.842175\n",
      "test_mean loss: 0.644928\n",
      "test_accuracy: 0.811183\n",
      "\n",
      "Start of epoch 9\n",
      "-----9------\n",
      "train_mean loss: 0.469997\n",
      "train_accuracy: 0.838719\n",
      "test_mean loss: 0.674941\n",
      "test_accuracy: 0.801053\n",
      "\n",
      "Start of epoch 10\n",
      "-----10------\n",
      "train_mean loss: 0.467284\n",
      "train_accuracy: 0.841463\n",
      "test_mean loss: 0.673289\n",
      "test_accuracy: 0.796191\n",
      "\n",
      "Start of epoch 11\n",
      "-----11------\n",
      "train_mean loss: 0.462720\n",
      "train_accuracy: 0.843191\n",
      "test_mean loss: 0.651766\n",
      "test_accuracy: 0.805511\n",
      "\n",
      "Start of epoch 12\n",
      "-----12------\n",
      "train_mean loss: 0.454436\n",
      "train_accuracy: 0.848171\n",
      "test_mean loss: 0.638379\n",
      "test_accuracy: 0.815235\n",
      "\n",
      "Start of epoch 13\n",
      "-----13------\n",
      "train_mean loss: 0.456858\n",
      "train_accuracy: 0.846850\n",
      "test_mean loss: 0.659859\n",
      "test_accuracy: 0.811588\n",
      "\n",
      "Start of epoch 14\n",
      "-----14------\n",
      "train_mean loss: 0.462179\n",
      "train_accuracy: 0.840346\n",
      "test_mean loss: 0.662057\n",
      "test_accuracy: 0.811588\n",
      "\n",
      "Start of epoch 15\n",
      "-----15------\n",
      "train_mean loss: 0.443215\n",
      "train_accuracy: 0.853557\n",
      "test_mean loss: 0.653427\n",
      "test_accuracy: 0.811183\n",
      "\n",
      "Start of epoch 16\n",
      "-----16------\n",
      "train_mean loss: 0.444945\n",
      "train_accuracy: 0.852439\n",
      "test_mean loss: 0.651953\n",
      "test_accuracy: 0.811994\n",
      "\n",
      "Start of epoch 17\n",
      "-----17------\n",
      "train_mean loss: 0.444442\n",
      "train_accuracy: 0.854167\n",
      "test_mean loss: 0.655061\n",
      "test_accuracy: 0.809157\n",
      "\n",
      "Start of epoch 18\n",
      "-----18------\n",
      "train_mean loss: 0.442328\n",
      "train_accuracy: 0.849390\n",
      "test_mean loss: 0.657199\n",
      "test_accuracy: 0.809157\n",
      "\n",
      "Start of epoch 19\n",
      "-----19------\n",
      "train_mean loss: 0.444524\n",
      "train_accuracy: 0.848781\n",
      "test_mean loss: 0.680933\n",
      "test_accuracy: 0.799028\n",
      "\n",
      "Start of epoch 20\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-4531e79b9c52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nStart of epoch %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mckpt_mana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# print(lr_schedule)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-8239ee8bb52f>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# print(jittered_data.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mcallmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjittered_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;31m# pred_val = np.argmax(logits, 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# correct = np.sum(pred_val == current_label[start_idx:end_idx])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    train_one_epoch(epoch)\n",
    "    ckpt_mana.save()\n",
    "    # print(lr_schedule)\n",
    "    log_string(\"-----{}------\".format(epoch))\n",
    "    log_string('train_mean loss: %f' % float(mtric2.result())) \n",
    "    log_string('train_accuracy: %f' % float(mtric1.result()))\n",
    "    mtric1.reset_states()\n",
    "    mtric2.reset_states()\n",
    "    test_one_epoch(epoch)\n",
    "    log_string('test_mean loss: %f' % float(mtric2.result()))\n",
    "    log_string('test_accuracy: %f' % float(mtric1.result()))\n",
    "    mtric1.reset_states()\n",
    "    mtric2.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(tf.train.latest_checkpoint('pointcloud_class'))\n",
    "model=ckpt.model\n",
    "optimizer=ckpt.opti\n",
    "BATCH=ckpt.batch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.00028337777>\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}